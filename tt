# coding: utf-8
import tensorflow as tf
import numpy as np
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import dok_matrix
from pandas import DataFrame
import pandas as pd
import random


class Graph(object):
    def __init__(self, config, buyer_cate):
        self.start = 0
        self.is_epoch_end = False
        self.buyer_seller_dict = {}
        self.struct_list = config.struct_list
        self.buyer_cate = buyer_cate

        self.le0 = LabelEncoder()
        self.le1 = LabelEncoder()

        self.le0.fit(self.buyer_cate['buyer_id'])
        self.le1.fit(self.buyer_cate['cate_id'])
        self.buyer_cate['buyer_id'] = self.le0.transform(self.buyer_cate['buyer_id'])
        self.buyer_cate['cate_id'] = self.le1.transform(self.buyer_cate['cate_id'])

        self.buyer_num = max(self.buyer_cate['buyer_id']) + 1
        self.cate_num = max(self.buyer_cate['cate_id']) + 1
        self.total_num = self.cate_num + self.buyer_num

        self.adj_matrix = dok_matrix((self.total_num, self.total_num), np.int_)

        for i in range(self.total_num):
            buyer_id = self.buyer_cate.loc[i]['buyer_id']
            cate_id = self.buyer_cate.loc[i]['cate_id']
            self.adj_matrix[buyer_id, cate_id + self.buyer_num] = 1
            self.adj_matrix[cate_id + self.buyer_num, buyer_id] = 1

        self.adj_matrix = self.adj_matrix.tocsr()
        self.N = self.total_num
        config.N = self.N
        self.E = self.adj_matrix.count_nonzero() / 2
        self.order = np.arange(self.total_num)
        print("Graph initialized! buyers:{}, cates:{}, all nodes:{}, edges:{}".format(
            self.buyer_num, self.cate_num, self.total_num, self.E))

    def sample(self, batch_size, do_shuffle=True):
        if self.is_epoch_end:
            if do_shuffle:
                np.random.shuffle(self.order[0:self.N])
            else:
                self.order = np.sort(self.order)
            self.start = 0
            self.is_epoch_end = False
        mini_batch = Dotdict()
        end = min(self.N, self.start + batch_size)
        index = self.order[self.start:end]
        mini_batch.X = self.adj_matrix[index].toarray()
        mini_batch.adjacent_matriX = self.adj_matrix[index].toarray()[:][:, index]
        if end == self.N:
            end = 0
            self.is_epoch_end = True
        self.start = end
        return mini_batch


class SDNE:
    def __init__(self, config):
        self.is_variables_init = False
        self.config = config
        self.sess = tf.Session()

        self.struct_list = config.struct_list
        self.layers = len(self.struct_list)
        self.W = {}
        self.b = {}
        struct_list = self.struct_list

        for i in range(self.layers - 1):
            name = "encoder" + str(i)
            self.W[name] = tf.Variable(tf.random_normal([struct_list[i], struct_list[i + 1]]), name=name)
            self.b[name] = tf.Variable(tf.zeros([struct_list[i + 1]]), name=name)
        struct_list.reverse()
        for i in range(self.layers - 1):
            name = "decoder" + str(i)
            self.W[name] = tf.Variable(tf.random_normal([struct_list[i], struct_list[i + 1]]), name=name)
            self.b[name] = tf.Variable(tf.zeros([struct_list[i + 1]]), name=name)
        self.struct_list.reverse()

        self.adjacent_matriX = tf.placeholder("float", [None, None])
        self.X = tf.placeholder("float", [None, struct_list[0]])

        self.__make_compute_graph()
        self.loss = self.__make_loss()
        self.optimizer = tf.train.RMSPropOptimizer(config.learning_rate).minimize(self.loss)

    def __make_compute_graph(self):
        def encoder(X):
            for i in range(self.layers - 1):
                name = "encoder" + str(i)
                X = tf.nn.sigmoid(tf.matmul(X, self.W[name]) + self.b[name])
            return X

        def decoder(X):
            for i in range(self.layers - 1):
                name = "decoder" + str(i)
                X = tf.nn.sigmoid(tf.matmul(X, self.W[name]) + self.b[name])
            return X

        self.H = encoder(self.X)
        self.X_reconstruct = decoder(self.H)

    def __make_loss(self):

        def get_1st_loss(H, adj_mini_batch):
            D = tf.diag(tf.reduce_sum(adj_mini_batch, 1))
            L = D - adj_mini_batch
            return 2 * tf.trace(tf.matmul(tf.matmul(tf.transpose(H), L), H))

        def get_2nd_loss(X, newX, beta):
            B = X * (beta - 1) + 1
            return tf.reduce_sum(tf.pow((newX - X) * B, 2))

        def get_reg_loss(weight, biases):
            ret = tf.add_n([tf.nn.l2_loss(w) for w in weight.values()])
            ret = ret + tf.add_n([tf.nn.l2_loss(b) for b in biases.values()])
            return ret

        # Loss function
        self.loss_2nd = get_2nd_loss(self.X, self.X_reconstruct, self.config.beta)
        self.loss_1st = get_1st_loss(self.H, self.adjacent_matriX)
        self.loss_reg = get_reg_loss(self.W, self.b)

        return self.config.gamma * self.loss_1st + self.config.alpha * self.loss_2nd + self.config.reg * self.loss_reg

    def do_variables_init(self, data):
        def assign(a, b):
            op = a.assign(b)
            self.sess.run(op)

        init = tf.global_variables_initializer()
        self.sess.run(init)

        if self.config.DBN_init:
            shape = self.struct_list
            myRBMs = []
            for i in range(len(shape) - 1):
                myRBM = rbm([shape[i], shape[i + 1]],
                            {'batch_size': self.config.DBN_batch_size, 'learning_rate': self.config.DBN_learning_rate})
                myRBMs.append(myRBM)
                for epoch in range(self.config.DBN_epochs):
                    error = 0
                    for batch in range(0, data.N, self.config.DBN_batch_size):
                        mini_batch = data.sample(self.config.DBN_batch_size).X
                        for k in range(len(myRBMs) - 1):
                            mini_batch = myRBMs[k].getH(mini_batch)
                        error += myRBM.fit(mini_batch)
                    print("rbm epochs:", epoch, "error : ", error)

                W, bv, bh = myRBM.getWb()
                name = "encoder" + str(i)
                assign(self.W[name], W)
                assign(self.b[name], bh)
                name = "decoder" + str(self.layers - i - 2)
                assign(self.W[name], W.transpose())
                assign(self.b[name], bv)
        self.is_Init = True

    def __get_feed_dict(self, data):
        return {self.X: data.X, self.adjacent_matriX: data.adjacent_matriX}

    def fit(self, data):
        feed_dict = self.__get_feed_dict(data)
        ret, _ = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)
        return ret

    def get_loss(self, data):
        feed_dict = self.__get_feed_dict(data)
        return self.sess.run(self.loss, feed_dict=feed_dict)

    def get_embedding(self, data):
        return self.sess.run(self.H, feed_dict=self.__get_feed_dict(data))

    def get_W(self):
        return self.sess.run(self.W)

    def get_B(self):
        return self.sess.run(self.b)

    def close(self):
        self.sess.close()


class rbm:
    def __init__(self, shape, para):
        # shape[0] means the number of visible units
        # shape[1] means the number of hidden units
        self.para = para
        self.sess = tf.Session()
        stddev = 1.0 / np.sqrt(shape[0])
        self.W = tf.Variable(tf.random_normal([shape[0], shape[1]], stddev=stddev), name="Wii")
        self.bv = tf.Variable(tf.zeros(shape[0]), name="a")
        self.bh = tf.Variable(tf.zeros(shape[1]), name="b")
        self.v = tf.placeholder("float", [None, shape[0]])
        init_op = tf.global_variables_initializer()
        self.sess.run(init_op)
        self.buildModel()
        print("rbm init completely")
        pass

    def buildModel(self):
        self.h = self.sample(tf.sigmoid(tf.matmul(self.v, self.W) + self.bh))
        # gibbs_sample
        v_sample = self.sample(tf.sigmoid(tf.matmul(self.h, tf.transpose(self.W)) + self.bv))
        h_sample = self.sample(tf.sigmoid(tf.matmul(v_sample, self.W) + self.bh))
        lr = self.para["learning_rate"] / tf.to_float(self.para["batch_size"])
        W_adder = self.W.assign_add(
            lr * (tf.matmul(tf.transpose(self.v), self.h) - tf.matmul(tf.transpose(v_sample), h_sample)))
        bv_adder = self.bv.assign_add(lr * tf.reduce_mean(self.v - v_sample, 0))
        bh_adder = self.bh.assign_add(lr * tf.reduce_mean(self.h - h_sample, 0))
        self.upt = [W_adder, bv_adder, bh_adder]
        self.error = tf.reduce_sum(tf.pow(self.v - v_sample, 2))

    def fit(self, data):
        _, ret = self.sess.run((self.upt, self.error), feed_dict={self.v: data})
        return ret

    def sample(self, probs):
        return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))

    def getWb(self):
        return self.sess.run([self.W, self.bv, self.bh])

    def getH(self, data):
        return self.sess.run(self.h, feed_dict={self.v: data})


class Dotdict(dict):
    """dot.notation access to dictionary attributes"""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


def add_layer(inputs, in_size, out_size, activation_function=None):
    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    return outputs


if __name__ == "__main__":

    config = Dotdict()
    config.alpha = 1
    config.gamma = 0.2
    config.reg = 0.1
    config.structure = '675, 64'
    config.struct_list = [int(x) for x in config.structure.split(',')]

    config.learning_rate = 0.001
    config.batch_size = 64
    config.beta = 2
    config.epochs_limit = 20
    config.display = 5

    config.DBN_init = False
    config.DBN_epochs = 120
    config.DBN_batch_size = 64
    config.DBN_learning_rate = 0.1

    input_table_path = "D:\PycharmProjects\zhutong_SDNE\Data\sdne_data_label.txt"
    output_table_path = "D:\PycharmProjects\zhutong_SDNE\Data"

    # 载入数据
    buyer_cate_label = pd.read_csv(input_table_path, sep='\t')
    buyer_cate = buyer_cate_label[['buyer_id', 'cate_id']]

    # 构建图
    train_graph_data = Graph(config, buyer_cate)

    # 构建模型
    model = SDNE(config)
    model.do_variables_init(train_graph_data)

    # 训练
    embedding = None
    batch_size = config.batch_size
    display = config.display
    while True:
        mini_batch = train_graph_data.sample(batch_size, do_shuffle=False)
        if embedding is None:
            embedding = model.get_embedding(mini_batch)
        else:
            embedding = np.vstack((embedding, model.get_embedding(mini_batch)))
        if train_graph_data.is_epoch_end:
            break

    epochs = 0

    while True:
        with model.sess.as_default():
            mini_batch = train_graph_data.sample(batch_size)
            loss = model.fit(mini_batch)
            if train_graph_data.is_epoch_end:
                epochs += 1
                loss = 0
                if epochs % display == 0:
                    embedding = None
                    while True:
                        mini_batch = train_graph_data.sample(batch_size, do_shuffle=False)
                        loss += model.get_loss(mini_batch)
                        if embedding is None:
                            embedding = model.get_embedding(mini_batch)
                        else:
                            embedding = np.vstack((embedding, model.get_embedding(mini_batch)))
                        if train_graph_data.is_epoch_end:
                            break
                    print("Epoch : %d loss : %.3f" % (epochs, loss))

                if epochs == config.epochs_limit:
                    print("exceed epochs limit terminating")
                    break

    # 分类器模型
    buyer_num = train_graph_data.buyer_num
    train_len = int(buyer_num*0.7)
    users = []
    for i in np.arange(buyer_num):
        user = train_graph_data.le0.inverse_transform(i)
        users.append([int(user)])
    res = np.hstack((users, embedding[:buyer_num]))
    res_cols = []
    res_cols.append('buyer_id')
    for i in range(64):
        name = 'c' + str(i)
        res_cols.append(name)
    res = DataFrame(res, columns=res_cols)

    data = pd.merge(res, buyer_cate_label, how='inner', on='buyer_id')

    order = np.arange(0, buyer_num)
    random.shuffle(order)
    train_order = order[:train_len].tolist()
    test_order = order[train_len: buyer_num].tolist()

    train_data = data.loc[train_order]
    test_data = data.loc[test_order]

    col = res_cols[1:]
    x_data = np.array(train_data[col])[1:]
    y_data = train_data['label']

    xs = tf.placeholder(tf.float32, [None, 64])
    ys = tf.placeholder(tf.float32,)

    # add hidden layer 输入值是 xs，在隐藏层有 5 个神经元
    l1 = add_layer(xs, 64, 5, activation_function=tf.nn.relu)
    # add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果
    prediction = add_layer(l1, 5, 1, activation_function=None)

    # the error between prediciton and real data
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                                        reduction_indices=[1]))
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)

    for i in range(buyer_num):
        for (x, y) in zip(x_data, y_data):
            x = x[:, np.newaxis]
            sess.run(train_step, feed_dict={xs: x.T, ys: y})
            if i % 50 == 0:
                print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))

    # 写表
    writer = tf.TableRecordWriter(output_table_path)
    cols = []
    cols.append(tf.placeholder(tf.string))
    for i in range(64):
        cols.append(tf.placeholder(tf.float32))
    users = []
    for i in np.arange(train_graph_data.buyer_num):
        user = train_graph_data.le0.inverse_transform(i)
        users.append([str(int(user))])
    print(embedding.shape)
    res = np.hstack((users, embedding[:train_graph_data.buyer_num]))
    res = res.T
    res.tolist()
    feed_dict = dict(zip(cols, res))
    write_to_table = writer.write(range(65), cols)
    close_table = writer.close()

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        sess.run(write_to_table, feed_dict=feed_dict)
        sess.run(close_table)
        coord.request_stop()
        coord.join(threads)
